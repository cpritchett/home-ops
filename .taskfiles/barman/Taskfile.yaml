---
version: "3"

# Taskfile for managing CloudNative-PG barman-cloud backup operations
# Provides utilities for listing, managing, and verifying PostgreSQL backups stored via barman-cloud

vars:
  BARMAN_CONTAINER_IMAGE: "ghcr.io/cloudnative-pg/plugin-barman-cloud:v0.5.0"
  POSTGRES_NAMESPACE: "cloudnative-pg"
  POSTGRES_CLUSTER: "postgres"

tasks:
  list-backups:
    desc: List all available PostgreSQL backups in the S3 destination
    cmds:
      - |
        echo "üì¶ Scanning PostgreSQL backup repositories..."
        echo "=============================================="

        # Get S3 credentials from the secret
        echo "Retrieving S3 credentials..."
        if ! kubectl get secret cloudnative-pg-secret -n {{.POSTGRES_NAMESPACE}} >/dev/null 2>&1; then
          echo "‚ùå Error: cloudnative-pg-secret not found in namespace {{.POSTGRES_NAMESPACE}}"
          exit 1
        fi

        AWS_ACCESS_KEY_ID=$(kubectl get secret cloudnative-pg-secret -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)
        AWS_SECRET_ACCESS_KEY=$(kubectl get secret cloudnative-pg-secret -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)

        if [ -z "$AWS_ACCESS_KEY_ID" ] || [ -z "$AWS_SECRET_ACCESS_KEY" ]; then
          echo "‚ùå Error: Could not retrieve S3 credentials from secret"
          exit 1
        fi

        # Get ObjectStore configuration
        if ! kubectl get objectstore s3 -n {{.POSTGRES_NAMESPACE}} >/dev/null 2>&1; then
          echo "‚ùå Error: ObjectStore 's3' not found in namespace {{.POSTGRES_NAMESPACE}}"
          exit 1
        fi

        ENDPOINT_URL=$(kubectl get objectstore s3 -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.spec.configuration.endpointURL}')
        DESTINATION_PATH=$(kubectl get objectstore s3 -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.spec.configuration.destinationPath}')

        echo "Configuration:"
        echo "  Endpoint: $ENDPOINT_URL"
        echo "  Destination: $DESTINATION_PATH"
        echo ""

        # Run barman-cloud-backup-list in a temporary pod
        echo "Fetching backup list..."
        kubectl run barman-list-temp --rm -i --restart=Never \
          --image={{.BARMAN_CONTAINER_IMAGE}} \
          --namespace={{.POSTGRES_NAMESPACE}} \
          --env="AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID" \
          --env="AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY" \
          --env="AWS_ENDPOINT_URL=$ENDPOINT_URL" \
          --command -- barman-cloud-backup-list \
          --endpoint-url="$ENDPOINT_URL" \
          "$DESTINATION_PATH" postgres-v23 2>/dev/null || {
          echo "‚ö†Ô∏è  No backups found or error accessing backup repository"
          echo "   This could mean:"
          echo "   - No backups have been created yet"
          echo "   - WAL archiving is disabled (check cluster.yaml)"
          echo "   - S3 credentials or configuration issues"
          exit 0
        }
    preconditions:
      - which kubectl
      - which base64

  list-clusters:
    desc: List all PostgreSQL clusters with backup configuration
    cmds:
      - |
        echo "üóÑÔ∏è  PostgreSQL Clusters with Backup Configuration"
        echo "================================================="

        clusters=$(kubectl get clusters.postgresql.cnpg.io --all-namespaces --no-headers --output=jsonpath='{range .items[*]}{.metadata.namespace},{.metadata.name}{"\n"}{end}' 2>/dev/null || echo "")

        if [ -z "$clusters" ]; then
          echo "No PostgreSQL clusters found"
          exit 0
        fi

        printf "%-20s %-15s %-10s %-15s %-20s %s\n" "CLUSTER" "NAMESPACE" "INSTANCES" "BACKUP_ENABLED" "SCHEDULED_BACKUP" "STATUS"
        printf "%-20s %-15s %-10s %-15s %-20s %s\n" "-------" "---------" "---------" "--------------" "----------------" "------"

        echo "$clusters" | while IFS=',' read -r namespace cluster; do
          if [ -n "$namespace" ] && [ -n "$cluster" ]; then
            # Get cluster info
            instances=$(kubectl get cluster $cluster -n $namespace -o jsonpath='{.spec.instances}' 2>/dev/null || echo "N/A")
            
            # Check if backup plugin is enabled
            backup_enabled=$(kubectl get cluster $cluster -n $namespace -o jsonpath='{.spec.plugins}' 2>/dev/null | grep -q "barman-cloud" && echo "‚úÖ Yes" || echo "‚ùå No")
            
            # Check for scheduled backup
            scheduled_backup=$(kubectl get scheduledbackup $cluster -n $namespace >/dev/null 2>&1 && echo "‚úÖ Configured" || echo "‚ùå Not configured")
            
            # Get cluster status
            status=$(kubectl get cluster $cluster -n $namespace -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
            
            printf "%-20s %-15s %-10s %-15s %-20s %s\n" "$cluster" "$namespace" "$instances" "$backup_enabled" "$scheduled_backup" "$status"
          fi
        done

        echo ""
        echo "Legend:"
        echo "  ‚úÖ Enabled/Configured | ‚ùå Disabled/Not configured"
    preconditions:
      - which kubectl

  backup-info:
    desc: Show detailed backup information for a specific backup [BACKUP_ID=required]
    cmds:
      - |
        if [ -z "{{.BACKUP_ID}}" ]; then
          echo "‚ùå Error: BACKUP_ID parameter is required"
          echo "Usage: task barman:backup-info BACKUP_ID=<backup-id>"
          echo "Use 'task barman:list-backups' to see available backup IDs"
          exit 1
        fi

        echo "üìã Backup Information for: {{.BACKUP_ID}}"
        echo "========================================"

        # Get S3 credentials
        AWS_ACCESS_KEY_ID=$(kubectl get secret cloudnative-pg-secret -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)
        AWS_SECRET_ACCESS_KEY=$(kubectl get secret cloudnative-pg-secret -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)
        ENDPOINT_URL=$(kubectl get objectstore s3 -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.spec.configuration.endpointURL}')
        DESTINATION_PATH=$(kubectl get objectstore s3 -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.spec.configuration.destinationPath}')

        echo "Fetching backup details..."
        kubectl run barman-info-temp --rm -i --restart=Never \
          --image={{.BARMAN_CONTAINER_IMAGE}} \
          --namespace={{.POSTGRES_NAMESPACE}} \
          --env="AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID" \
          --env="AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY" \
          --env="AWS_ENDPOINT_URL=$ENDPOINT_URL" \
          --command -- barman-cloud-backup-show \
          --endpoint-url="$ENDPOINT_URL" \
          "$DESTINATION_PATH" postgres-v23 {{.BACKUP_ID}} || {
          echo "‚ùå Error: Could not retrieve backup information for {{.BACKUP_ID}}"
          echo "   Ensure the backup ID is correct and accessible"
          exit 1
        }
    requires:
      vars: [BACKUP_ID]
    preconditions:
      - which kubectl
      - which base64

  restore-cluster:
    desc: Restore PostgreSQL cluster from backup [BACKUP_ID=required] [CLUSTER_NAME={{.CLUSTER_NAME}}] [DRY_RUN=false]
    cmds:
      - |
        if [ -z "{{.BACKUP_ID}}" ]; then
          echo "‚ùå Error: BACKUP_ID parameter is required"
          echo "Usage: task barman:restore-cluster BACKUP_ID=<backup-id> [CLUSTER_NAME=<name>] [DRY_RUN=true]"
          exit 1
        fi

        CLUSTER_NAME="{{.CLUSTER_NAME}}"
        DRY_RUN="{{.DRY_RUN}}"

        if [ -z "$CLUSTER_NAME" ]; then
          CLUSTER_NAME="{{.POSTGRES_CLUSTER}}-restore-$(date +%Y%m%d-%H%M%S)"
        fi

        echo "üîÑ PostgreSQL Cluster Restore"
        echo "============================="
        echo "Source backup: {{.BACKUP_ID}}"
        echo "New cluster name: $CLUSTER_NAME"
        echo "Namespace: {{.POSTGRES_NAMESPACE}}"
        echo "Dry run: $DRY_RUN"
        echo ""

        if [ "$DRY_RUN" = "true" ]; then
          echo "üß™ DRY RUN MODE - No actual restore will be performed"
          echo ""
        fi

        # Verify backup exists
        echo "Verifying backup exists..."
        AWS_ACCESS_KEY_ID=$(kubectl get secret cloudnative-pg-secret -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)
        AWS_SECRET_ACCESS_KEY=$(kubectl get secret cloudnative-pg-secret -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)
        ENDPOINT_URL=$(kubectl get objectstore s3 -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.spec.configuration.endpointURL}')
        DESTINATION_PATH=$(kubectl get objectstore s3 -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.spec.configuration.destinationPath}')

        if ! kubectl run barman-verify-temp --rm -i --restart=Never \
          --image={{.BARMAN_CONTAINER_IMAGE}} \
          --namespace={{.POSTGRES_NAMESPACE}} \
          --env="AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID" \
          --env="AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY" \
          --env="AWS_ENDPOINT_URL=$ENDPOINT_URL" \
          --command -- barman-cloud-backup-show \
          --endpoint-url="$ENDPOINT_URL" \
          "$DESTINATION_PATH" postgres-v23 {{.BACKUP_ID}} >/dev/null 2>&1; then
          echo "‚ùå Error: Backup {{.BACKUP_ID}} not found or not accessible"
          exit 1
        fi

        echo "‚úÖ Backup {{.BACKUP_ID}} verified"

        if [ "$DRY_RUN" = "true" ]; then
          echo ""
          echo "üìã Restore Plan:"
          echo "  1. Create new cluster manifest with bootstrap.recovery configuration"
          echo "  2. Apply cluster manifest to create restored cluster"
          echo "  3. Wait for cluster to become ready"
          echo "  4. Verify data integrity"
          echo ""
          echo "To execute the restore, run:"
          echo "task barman:restore-cluster BACKUP_ID={{.BACKUP_ID}} CLUSTER_NAME=$CLUSTER_NAME DRY_RUN=false"
          exit 0
        fi

        # Confirm destructive operation
        echo ""
        echo "‚ö†Ô∏è  WARNING: This will create a new PostgreSQL cluster"
        read -p "Continue with restore? (y/N): " confirm
        if [[ ! "$confirm" =~ ^[Yy]$ ]]; then
          echo "Operation cancelled"
          exit 1
        fi

        # Create restore cluster manifest
        echo "Creating restore cluster manifest..."
        cat <<EOF | kubectl apply -f -
        apiVersion: postgresql.cnpg.io/v1
        kind: Cluster
        metadata:
          name: $CLUSTER_NAME
          namespace: {{.POSTGRES_NAMESPACE}}
        spec:
          instances: 1
          imageName: ghcr.io/cloudnative-pg/postgresql:\${POSTGRESQL_VERSION}
          storage:
            size: 20Gi
            storageClass: openebs-hostpath
          bootstrap:
            recovery:
              source: restore-source
              recoveryTarget:
                backupID: "{{.BACKUP_ID}}"
          externalClusters:
            - name: restore-source
              plugin:
                name: barman-cloud.cloudnative-pg.io
                parameters:
                  barmanObjectName: s3
                  serverName: "postgres-v23"
        EOF

        echo "‚úÖ Cluster manifest applied"
        echo "Waiting for cluster to become ready..."

        kubectl wait --for=condition=Ready cluster/$CLUSTER_NAME -n {{.POSTGRES_NAMESPACE}} --timeout=600s || {
          echo "‚ùå Error: Cluster failed to become ready within 10 minutes"
          echo "Check cluster status with: kubectl describe cluster $CLUSTER_NAME -n {{.POSTGRES_NAMESPACE}}"
          exit 1
        }

        echo "‚úÖ Cluster restore completed successfully!"
        echo "New cluster: $CLUSTER_NAME"
        echo "Check status with: kubectl get cluster $CLUSTER_NAME -n {{.POSTGRES_NAMESPACE}}"
    vars:
      CLUSTER_NAME: '{{.CLUSTER_NAME | default ""}}'
      DRY_RUN: '{{.DRY_RUN | default "false"}}'
    requires:
      vars: [BACKUP_ID]
    preconditions:
      - which kubectl
      - which base64

  delete-backup:
    desc: Delete a specific backup [BACKUP_ID=required]
    cmds:
      - |
        if [ -z "{{.BACKUP_ID}}" ]; then
          echo "‚ùå Error: BACKUP_ID parameter is required"
          echo "Usage: task barman:delete-backup BACKUP_ID=<backup-id>"
          exit 1
        fi

        echo "üóëÔ∏è  Delete PostgreSQL Backup"
        echo "============================"
        echo "Backup ID: {{.BACKUP_ID}}"
        echo ""
        echo "‚ö†Ô∏è  WARNING: This action is IRREVERSIBLE!"

        # Get credentials and verify backup exists first
        AWS_ACCESS_KEY_ID=$(kubectl get secret cloudnative-pg-secret -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)
        AWS_SECRET_ACCESS_KEY=$(kubectl get secret cloudnative-pg-secret -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)
        ENDPOINT_URL=$(kubectl get objectstore s3 -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.spec.configuration.endpointURL}')
        DESTINATION_PATH=$(kubectl get objectstore s3 -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.spec.configuration.destinationPath}')

        echo "Verifying backup exists..."
        if ! kubectl run barman-verify-temp --rm -i --restart=Never \
          --image={{.BARMAN_CONTAINER_IMAGE}} \
          --namespace={{.POSTGRES_NAMESPACE}} \
          --env="AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID" \
          --env="AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY" \
          --env="AWS_ENDPOINT_URL=$ENDPOINT_URL" \
          --command -- barman-cloud-backup-show \
          --endpoint-url="$ENDPOINT_URL" \
          "$DESTINATION_PATH" postgres-v23 {{.BACKUP_ID}} >/dev/null 2>&1; then
          echo "‚ùå Error: Backup {{.BACKUP_ID}} not found"
          exit 1
        fi

        read -p "Are you sure you want to delete backup {{.BACKUP_ID}}? (y/N): " confirm
        if [[ ! "$confirm" =~ ^[Yy]$ ]]; then
          echo "Operation cancelled"
          exit 1
        fi

        echo "Deleting backup {{.BACKUP_ID}}..."
        kubectl run barman-delete-temp --rm -i --restart=Never \
          --image={{.BARMAN_CONTAINER_IMAGE}} \
          --namespace={{.POSTGRES_NAMESPACE}} \
          --env="AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID" \
          --env="AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY" \
          --env="AWS_ENDPOINT_URL=$ENDPOINT_URL" \
          --command -- barman-cloud-backup-delete \
          --endpoint-url="$ENDPOINT_URL" \
          "$DESTINATION_PATH" postgres-v23 {{.BACKUP_ID}} || {
          echo "‚ùå Error: Failed to delete backup {{.BACKUP_ID}}"
          exit 1
        }

        echo "‚úÖ Backup {{.BACKUP_ID}} deleted successfully"
    requires:
      vars: [BACKUP_ID]
    preconditions:
      - which kubectl
      - which base64

  delete-old-backups:
    desc: Delete backups older than specified days [DAYS=required]
    cmds:
      - |
        if [ -z "{{.DAYS}}" ]; then
          echo "‚ùå Error: DAYS parameter is required"
          echo "Usage: task barman:delete-old-backups DAYS=<number-of-days>"
          echo "Example: task barman:delete-old-backups DAYS=30"
          exit 1
        fi

        echo "üóëÔ∏è  Delete Old PostgreSQL Backups"
        echo "================================="
        echo "Retention: Keep backups newer than {{.DAYS}} days"
        echo ""

        # Get credentials
        AWS_ACCESS_KEY_ID=$(kubectl get secret cloudnative-pg-secret -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)
        AWS_SECRET_ACCESS_KEY=$(kubectl get secret cloudnative-pg-secret -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)
        ENDPOINT_URL=$(kubectl get objectstore s3 -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.spec.configuration.endpointURL}')
        DESTINATION_PATH=$(kubectl get objectstore s3 -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.spec.configuration.destinationPath}')

        # Get list of backups to delete
        echo "Scanning for backups older than {{.DAYS}} days..."

        cutoff_date=$(date -d "{{.DAYS}} days ago" +%Y%m%d)
        echo "Cutoff date: $cutoff_date"
        echo ""

        # List backups and identify old ones
        backups_to_delete=$(kubectl run barman-list-temp --rm -i --restart=Never \
          --image={{.BARMAN_CONTAINER_IMAGE}} \
          --namespace={{.POSTGRES_NAMESPACE}} \
          --env="AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID" \
          --env="AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY" \
          --env="AWS_ENDPOINT_URL=$ENDPOINT_URL" \
          --command -- barman-cloud-backup-list \
          --endpoint-url="$ENDPOINT_URL" \
          "$DESTINATION_PATH" postgres-v23 2>/dev/null | \
          awk -v cutoff="$cutoff_date" '
          /^[0-9]{8}T[0-9]{6}/ {
            backup_date = substr($1, 1, 8)
            if (backup_date < cutoff) {
              print $1
            }
          }' || echo "")

        if [ -z "$backups_to_delete" ]; then
          echo "‚úÖ No backups older than {{.DAYS}} days found"
          exit 0
        fi

        echo "üìã Backups to be deleted:"
        echo "$backups_to_delete"
        echo ""

        backup_count=$(echo "$backups_to_delete" | wc -l)
        echo "Total backups to delete: $backup_count"
        echo ""
        echo "‚ö†Ô∏è  WARNING: This action is IRREVERSIBLE!"
        read -p "Continue with deletion? (y/N): " confirm
        if [[ ! "$confirm" =~ ^[Yy]$ ]]; then
          echo "Operation cancelled"
          exit 1
        fi

        # Delete backups one by one
        success_count=0
        error_count=0

        echo "$backups_to_delete" | while read -r backup_id; do
          if [ -n "$backup_id" ]; then
            echo "Deleting backup: $backup_id"
            if kubectl run barman-delete-temp --rm -i --restart=Never \
              --image={{.BARMAN_CONTAINER_IMAGE}} \
              --namespace={{.POSTGRES_NAMESPACE}} \
              --env="AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID" \
              --env="AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY" \
              --env="AWS_ENDPOINT_URL=$ENDPOINT_URL" \
              --command -- barman-cloud-backup-delete \
              --endpoint-url="$ENDPOINT_URL" \
              "$DESTINATION_PATH" postgres-v23 "$backup_id" >/dev/null 2>&1; then
              echo "  ‚úÖ Deleted: $backup_id"
              success_count=$((success_count + 1))
            else
              echo "  ‚ùå Failed: $backup_id"
              error_count=$((error_count + 1))
            fi
          fi
        done

        echo ""
        echo "=== DELETION SUMMARY ==="
        echo "Successfully deleted: $success_count"
        echo "Failed deletions: $error_count"
        echo "========================"
    requires:
      vars: [DAYS]
    preconditions:
      - which kubectl
      - which base64
      - which date
      - which awk

  trigger-backup:
    desc: Trigger an immediate backup for the PostgreSQL cluster [CLUSTER={{.POSTGRES_CLUSTER}}]
    cmds:
      - |
        CLUSTER="{{.CLUSTER}}"
        if [ -z "$CLUSTER" ]; then
          CLUSTER="{{.POSTGRES_CLUSTER}}"
        fi

        echo "üîÑ Triggering PostgreSQL Backup"
        echo "==============================="
        echo "Cluster: $CLUSTER"
        echo "Namespace: {{.POSTGRES_NAMESPACE}}"
        echo ""

        # Check if cluster exists
        if ! kubectl get cluster $CLUSTER -n {{.POSTGRES_NAMESPACE}} >/dev/null 2>&1; then
          echo "‚ùå Error: Cluster '$CLUSTER' not found in namespace {{.POSTGRES_NAMESPACE}}"
          exit 1
        fi

        # Check if backup plugin is enabled
        if ! kubectl get cluster $CLUSTER -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.spec.plugins}' | grep -q "barman-cloud"; then
          echo "‚ùå Error: Barman-cloud plugin not enabled for cluster '$CLUSTER'"
          echo "   Enable WAL archiving in the cluster configuration first"
          exit 1
        fi

        # Create a manual backup
        backup_name="$CLUSTER-manual-$(date +%Y%m%d-%H%M%S)"

        echo "Creating backup: $backup_name"
        cat <<EOF | kubectl apply -f -
        apiVersion: postgresql.cnpg.io/v1
        kind: Backup
        metadata:
          name: $backup_name
          namespace: {{.POSTGRES_NAMESPACE}}
        spec:
          cluster:
            name: $CLUSTER
          method: plugin
          pluginConfiguration:
            name: barman-cloud.cloudnative-pg.io
        EOF

        echo "‚úÖ Backup '$backup_name' created"
        echo ""
        echo "Monitoring backup progress..."

        # Wait for backup to complete
        timeout=1800  # 30 minutes
        elapsed=0
        while [ $elapsed -lt $timeout ]; do
          status=$(kubectl get backup $backup_name -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.status.phase}' 2>/dev/null || echo "")
          
          case "$status" in
            "completed")
              echo "‚úÖ Backup completed successfully!"
              backup_id=$(kubectl get backup $backup_name -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.status.backupId}' 2>/dev/null || echo "unknown")
              echo "Backup ID: $backup_id"
              exit 0
              ;;
            "failed")
              echo "‚ùå Backup failed!"
              kubectl describe backup $backup_name -n {{.POSTGRES_NAMESPACE}}
              exit 1
              ;;
            "running")
              echo "‚è≥ Backup in progress... (${elapsed}s elapsed)"
              ;;
            *)
              echo "üìã Backup status: $status (${elapsed}s elapsed)"
              ;;
          esac
          
          sleep 30
          elapsed=$((elapsed + 30))
        done

        echo "‚è∞ Backup timed out after 30 minutes"
        echo "Check backup status with: kubectl get backup $backup_name -n {{.POSTGRES_NAMESPACE}}"
        exit 1
    vars:
      CLUSTER: '{{.CLUSTER | default ""}}'
    preconditions:
      - which kubectl
      - which date

  verify-backups:
    desc: Verify accessibility and integrity of all PostgreSQL backups
    cmds:
      - |
        echo "üîç Verifying PostgreSQL Backup Integrity"
        echo "========================================"

        # Get credentials
        AWS_ACCESS_KEY_ID=$(kubectl get secret cloudnative-pg-secret -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)
        AWS_SECRET_ACCESS_KEY=$(kubectl get secret cloudnative-pg-secret -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)
        ENDPOINT_URL=$(kubectl get objectstore s3 -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.spec.configuration.endpointURL}')
        DESTINATION_PATH=$(kubectl get objectstore s3 -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.spec.configuration.destinationPath}')

        echo "Configuration:"
        echo "  Endpoint: $ENDPOINT_URL"
        echo "  Destination: $DESTINATION_PATH"
        echo ""

        # Test S3 connectivity
        echo "üîó Testing S3 connectivity..."
        if kubectl run barman-test-temp --rm -i --restart=Never \
          --image={{.BARMAN_CONTAINER_IMAGE}} \
          --namespace={{.POSTGRES_NAMESPACE}} \
          --env="AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID" \
          --env="AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY" \
          --env="AWS_ENDPOINT_URL=$ENDPOINT_URL" \
          --command -- barman-cloud-backup-list \
          --endpoint-url="$ENDPOINT_URL" \
          "$DESTINATION_PATH" postgres-v23 >/dev/null 2>&1; then
          echo "  ‚úÖ S3 connectivity successful"
        else
          echo "  ‚ùå S3 connectivity failed"
          echo "     Check credentials and endpoint configuration"
          exit 1
        fi

        # Get backup list
        echo ""
        echo "üìã Listing available backups..."
        backup_list=$(kubectl run barman-list-temp --rm -i --restart=Never \
          --image={{.BARMAN_CONTAINER_IMAGE}} \
          --namespace={{.POSTGRES_NAMESPACE}} \
          --env="AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID" \
          --env="AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY" \
          --env="AWS_ENDPOINT_URL=$ENDPOINT_URL" \
          --command -- barman-cloud-backup-list \
          --endpoint-url="$ENDPOINT_URL" \
          "$DESTINATION_PATH" postgres-v23 2>/dev/null || echo "")

        if [ -z "$backup_list" ]; then
          echo "  üì≠ No backups found"
          echo ""
          echo "=== VERIFICATION SUMMARY ==="
          echo "S3 connectivity: ‚úÖ Success"
          echo "Backups found: 0"
          echo "============================"
          exit 0
        fi

        # Count backups
        backup_count=$(echo "$backup_list" | wc -l)
        echo "  Found $backup_count backup(s)"

        # Verify each backup (sample a few for large lists)
        if [ "$backup_count" -gt 5 ]; then
          echo ""
          echo "üîç Verifying sample of backups (first 3 and last 2)..."
          sample_backups=$(echo "$backup_list" | head -3; echo "$backup_list" | tail -2)
          verified_count=5
        else
          echo ""
          echo "üîç Verifying all backups..."
          sample_backups="$backup_list"
          verified_count=$backup_count
        fi

        success_count=0
        error_count=0

        echo "$sample_backups" | while read -r backup_line; do
          if [ -n "$backup_line" ]; then
            backup_id=$(echo "$backup_line" | awk '{print $1}')
            if [ -n "$backup_id" ]; then
              echo "  Verifying backup: $backup_id"
              if kubectl run barman-verify-temp --rm -i --restart=Never \
                --image={{.BARMAN_CONTAINER_IMAGE}} \
                --namespace={{.POSTGRES_NAMESPACE}} \
                --env="AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID" \
                --env="AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY" \
                --env="AWS_ENDPOINT_URL=$ENDPOINT_URL" \
                --command -- barman-cloud-backup-show \
                --endpoint-url="$ENDPOINT_URL" \
                "$DESTINATION_PATH" postgres-v23 "$backup_id" >/dev/null 2>&1; then
                echo "    ‚úÖ Backup accessible"
                success_count=$((success_count + 1))
              else
                echo "    ‚ùå Backup verification failed"
                error_count=$((error_count + 1))
              fi
            fi
          fi
        done

        echo ""
        echo "=== VERIFICATION SUMMARY ==="
        echo "S3 connectivity: ‚úÖ Success"
        echo "Total backups: $backup_count"
        echo "Verified backups: $verified_count"
        echo "Successful verifications: $success_count"
        echo "Failed verifications: $error_count"
        echo "============================"

        if [ "$error_count" -gt 0 ]; then
          echo ""
          echo "‚ö†Ô∏è  Some backups failed verification"
          echo "   Check individual backups with 'task barman:backup-info BACKUP_ID=<id>'"
          exit 1
        else
          echo ""
          echo "‚úÖ All verified backups are accessible!"
        fi
    preconditions:
      - which kubectl
      - which base64
      - which awk

  enable-wal-archiving:
    desc: Enable WAL archiving for PostgreSQL cluster [CLUSTER={{.POSTGRES_CLUSTER}}]
    cmds:
      - |
        CLUSTER="{{.CLUSTER}}"
        if [ -z "$CLUSTER" ]; then
          CLUSTER="{{.POSTGRES_CLUSTER}}"
        fi

        echo "üîß Enabling WAL Archiving for PostgreSQL Cluster"
        echo "==============================================="
        echo "Cluster: $CLUSTER"
        echo "Namespace: {{.POSTGRES_NAMESPACE}}"
        echo ""

        # Check if cluster exists
        if ! kubectl get cluster $CLUSTER -n {{.POSTGRES_NAMESPACE}} >/dev/null 2>&1; then
          echo "‚ùå Error: Cluster '$CLUSTER' not found in namespace {{.POSTGRES_NAMESPACE}}"
          exit 1
        fi

        # Check if already enabled
        if kubectl get cluster $CLUSTER -n {{.POSTGRES_NAMESPACE}} -o jsonpath='{.spec.plugins}' | grep -q "barman-cloud"; then
          echo "‚úÖ WAL archiving already enabled for cluster '$CLUSTER'"
          exit 0
        fi

        echo "‚ö†Ô∏è  WARNING: This will restart the PostgreSQL cluster"
        echo "The cluster will be temporarily unavailable during the restart"
        read -p "Continue with enabling WAL archiving? (y/N): " confirm
        if [[ ! "$confirm" =~ ^[Yy]$ ]]; then
          echo "Operation cancelled"
          exit 1
        fi

        echo "Enabling WAL archiving plugin..."
        kubectl patch cluster $CLUSTER -n {{.POSTGRES_NAMESPACE}} --type='merge' -p='
        {
          "spec": {
            "plugins": [
              {
                "name": "barman-cloud.cloudnative-pg.io",
                "isWALArchiver": true,
                "parameters": {
                  "barmanObjectName": "s3",
                  "serverName": "postgres-v23"
                }
              }
            ]
          }
        }'

        echo "‚úÖ WAL archiving plugin configuration applied"
        echo ""
        echo "Waiting for cluster to restart and become ready..."

        # Wait for cluster to become ready
        if kubectl wait --for=condition=Ready cluster/$CLUSTER -n {{.POSTGRES_NAMESPACE}} --timeout=600s; then
          echo "‚úÖ Cluster is ready with WAL archiving enabled"
          echo ""
          echo "Next steps:"
          echo "1. Trigger an initial backup: task barman:trigger-backup"
          echo "2. Verify backups are working: task barman:list-backups"
        else
          echo "‚ùå Cluster failed to become ready within 10 minutes"
          echo "Check cluster status with: kubectl describe cluster $CLUSTER -n {{.POSTGRES_NAMESPACE}}"
          exit 1
        fi
    vars:
      CLUSTER: '{{.CLUSTER | default ""}}'
    preconditions:
      - which kubectl
