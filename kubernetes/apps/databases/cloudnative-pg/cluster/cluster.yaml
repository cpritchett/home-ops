---
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgres
  annotations:
    cnpg.io/skipEmptyWalArchiveCheck: "enabled"
spec:
  instances: 3
  imageName: ghcr.io/cloudnative-pg/postgresql:${POSTGRESQL_VERSION}
  primaryUpdateStrategy: unsupervised
  storage:
    size: 20Gi
    storageClass: openebs-hostpath
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: postgres.priority
            operator: In
            values: ["preferred"]
      - weight: 30
        preference:
          matchExpressions:
          - key: postgres.priority
            operator: In
            values: ["fallback"]
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: postgres.priority
            operator: NotIn
            values: ["disabled"]
  superuserSecret:
    name: cloudnative-pg-secret
  enableSuperuserAccess: true
  postgresql:
    parameters:
      max_connections: "200"
      shared_buffers: "256MB"
  resources:
    requests:
      cpu: 200m  # Reduced from 300m to help with resource constraints. TODO: Restore to 300m when cluster resource usage (CPU < 80%, memory < 80%, no frequent pod restarts, and I/O wait times normalized) has stabilized for at least 24 hours. Monitor with Prometheus/Grafana dashboards and review incident #1234 for details.
      memory: 1Gi
    limits:
      memory: 2Gi
  monitoring:
    enablePodMonitor: true
  plugins:
    - name: barman-cloud.cloudnative-pg.io
      isWALArchiver: true
      parameters: &parameters
        barmanObjectName: s3
        serverName: "postgres-v23"
  # Note: uncomment bootstrap section when recovering from an existing cluster
  # bootstrap:
  #   recovery:
  #     source: source
  # externalClusters:
  #   - name: source
  #     plugin:
  #       name: barman-cloud.cloudnative-pg.io
  #       parameters: *parameters
